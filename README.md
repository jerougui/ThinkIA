# THINKIA â€“ Exemples d'Agents IA en mode Crew

Ce projet prÃ©sente des exemples concrets d'utilisation des techniques d'agents IA organisÃ©s en mode **Crew**, câ€™est-Ã -dire des agents collaborant pour accomplir des tÃ¢ches complexes de maniÃ¨re autonome et coordonnÃ©e.

## ğŸ“ Structure du projet
```
THINKIA/
â”œâ”€â”€ config/
â”œâ”€â”€ iapps/
â”‚   â””â”€â”€ diagnostic/
â”‚       â”œâ”€â”€ diagnostic_ia_agents.py
â”‚       â””â”€â”€ readme.md
â”œâ”€â”€ i-search/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ search_agent.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ sys_msgs.py
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ img/
â”‚       â”œâ”€â”€ console_example01.png
â”‚       â””â”€â”€ schema_diagram.png
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ollama_check.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ gemini_provider.py
â”‚   â”œâ”€â”€ llm_provider.py
â”‚   â”œâ”€â”€ ollama_manager.py
â”‚   â”œâ”€â”€ ollama_provider.py
â”‚   â”œâ”€â”€ openrouter_provider.py
â”‚   â””â”€â”€ sablier.py
â”œâ”€â”€ .env
â””â”€â”€ .gitignore

```
# Avant de commencer

## PrÃ©requis :
 [Python 3.10+](https://www.python.org/downloads/)
 ## ğŸ“– Documentation Ollama

Pour dÃ©marrer rapidement avec Ollama et exÃ©cuter des modÃ¨les localement, consulte la section [Quickstart de la documentation officielle](https://github.com/ollama/ollama/blob/main/README.md#quickstart) sur GitHub.

## ğŸš€ Documentation OpenRouter

Pour intÃ©grer rapidement OpenRouter et accÃ©der Ã  des centaines de modÃ¨les via une API unifiÃ©e, consulte le [guide Quickstart officiel](https://openrouter.ai/docs/quickstart).

 ## QuickStart
```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/jerougui/ThinkIA.git
cd ThinkIA

```
ğŸ” Configuration des clÃ©s API
Pour que lâ€™agent fonctionne correctement, tu dois crÃ©er un fichier .env Ã  la racine du projet. Ce fichier contient les clÃ©s API privÃ©es nÃ©cessaires Ã  la communication avec les modÃ¨les dâ€™IA.

ğŸ“„ Exemple de fichier `.env :
```text
# ğŸ” ClÃ©s API
GEMINI_API_KEY=<GEMINI_API_KEY>
OPENROUTER_API_KEY=<OPENROUTER_API_KEY>
````

## Installer les dÃ©pendances
```bash
pip install -r requirements.txt

python -m pip install -r requirements.txt
```
---

## ğŸš€ Lancer les exemples

### ğŸ©º Diagnostic mÃ©dical assistÃ© par agents IA
```bash
python iapps/diagnostic/diagnostic_ia_agents.py
```
â¡ï¸ Voir [iapps/diagnostic/readme.md](iapps/diagnostic/readme.md) pour plus d'informations.

## ğŸ” Recherche intelligente par agents IA
```bash
python iapps/i-search/search_agent.py
```
â¡ï¸ Voir [iapps/i-search/readme.md](iapps/i-search/readme.md) pour plus d'informations.

## ğŸ¯ Objectif du projet
DÃ©montrer comment des agents IA peuvent Ãªtre orchestrÃ©s en mode Crew pour :

Simuler des environnements intelligents

RÃ©aliser des tÃ¢ches complexes en autonomie

Collaborer via des rÃ´les spÃ©cialisÃ©s

## ğŸ› ï¸ Technologies utilisÃ©es
Python

ModÃ¨les LLM via ollama, openrouter, etc.

Architecture multi-agent Crew

Prompts dynamiques et contextuels

## ğŸ“š Ressources
Chaque exemple contient son propre readme.md avec des dÃ©tails sur :

Le rÃ´le des agents

Le workflow collaboratif

Les cas dâ€™usage simulÃ©s

## ğŸ› ï¸ Technologies utilisÃ©es

- Agents IA orchestrÃ©s en mode Crew
- IntÃ©gration avec des LLM en local Ã  l'aide de `ollama` ou  via des providers (`gemini`, `openrouter`, etc.)
- Prompts dynamiques et messages systÃ¨me personnalisÃ©s
- Scripts Python modulaires et testables

## ğŸ¤– Utilisation des modÃ¨les LLM

Les agents IA de ce projet s'appuient sur des **modÃ¨les de langage (LLM)** accessibles via deux modes :

### ğŸ” Mode local (exÃ©cution privÃ©e garantie)
Les modÃ¨les sont exÃ©cutÃ©s **en local** via des outils comme `Ollama`, garantissant :
- Aucune donnÃ©e envoyÃ©e Ã  des serveurs externes
- ContrÃ´le total sur les interactions et les logs
- ConfidentialitÃ© maximale pour les scÃ©narios sensibles

### ğŸŒ Mode provider (accÃ¨s Ã  des modÃ¨les distants)
Les agents peuvent Ã©galement interagir avec des **providers externes** comme `OpenRouter`, permettant :
- AccÃ¨s Ã  des modÃ¨les avancÃ©s hÃ©bergÃ©s Ã  distance
- FlexibilitÃ© dans le choix des capacitÃ©s et des coÃ»ts
- AdaptabilitÃ© selon les besoins du projet

> Le mode d'exÃ©cution (local ou provider) peut Ãªtre configurÃ© dynamiquement selon le contexte ou les prÃ©fÃ©rences de l'utilisateur.

## ğŸ¯ Objectif

Explorer et dÃ©montrer comment des agents IA peuvent collaborer efficacement dans des contextes variÃ©s, en simulant des environnements rÃ©els et des workflows intelligents.

---

